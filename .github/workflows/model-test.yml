name: Model Testing & Validation

on:
  # Trigger on code changes
  push:
    branches: [ main, develop ]
    paths:
      - 'ml/**'
      - '.github/workflows/model-test.yml'
  
  pull_request:
    branches: [ main ]
    paths:
      - 'ml/**'
  
  # Manual trigger
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  HF_MODEL_REPO: 'SilentStorm99/opspilot-phi3-lora-v6'

jobs:
  # Stage 1: Download model from HuggingFace and run tests
  test-model:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-ml-${{ hashFiles('ml/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        pip install -r ml/requirements.txt
        pip install pytest pytest-cov huggingface_hub
    
    - name: Download model from HuggingFace
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        cd ml
        echo "üì¶ Downloading trained model from HuggingFace..."
        python -c "
        from huggingface_hub import snapshot_download
        import os
        
        model_path = snapshot_download(
            repo_id='${{ env.HF_MODEL_REPO }}',
            token=os.getenv('HF_TOKEN'),
            local_dir='./outputs/lora_phi3_v6/final',
            local_dir_use_symlinks=False
        )
        print(f'‚úÖ Model downloaded to: {model_path}')
        "
        
        echo "üìÇ Model contents:"
        ls -lh ./outputs/lora_phi3_v6/final/
    
    - name: Verify model files
      run: |
        cd ml
        echo "üîç Verifying model files..."
        
        if [ ! -f "./outputs/lora_phi3_v6/final/adapter_model.safetensors" ]; then
          echo "‚ùå Error: adapter_model.safetensors not found"
          exit 1
        fi
        
        if [ ! -f "./outputs/lora_phi3_v6/final/adapter_config.json" ]; then
          echo "‚ùå Error: adapter_config.json not found"
          exit 1
        fi
        
        echo "‚úÖ All required model files present"
    
    - name: Run model inference tests
      env:
        PHI3_MODEL_PATH: "microsoft/Phi-3-mini-4k-instruct"
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        cd ml
        echo "üß™ Running inference tests..."
        python -c "
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import PeftModel
        import os
        
        print('üîß Loading base model...')
        base_model = AutoModelForCausalLM.from_pretrained(
            'microsoft/Phi-3-mini-4k-instruct',
            device_map='cpu',
            trust_remote_code=True,
            torch_dtype=torch.float32
        )
        
        print('üîß Loading LoRA adapters...')
        model = PeftModel.from_pretrained(
            base_model,
            './outputs/lora_phi3_v6/final',
            device_map='cpu'
        )
        
        print('üîß Loading tokenizer...')
        tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct')
        
        # Test inference
        test_prompt = '''<|system|>You are an IT incident classification assistant.<|end|>
        <|user|>Classify this incident: Database connection timeout<|end|>
        <|assistant|>'''
        
        print('üß™ Running test inference...')
        inputs = tokenizer(test_prompt, return_tensors='pt')
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,
            temperature=0.7,
            do_sample=True
        )
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f'‚úÖ Inference successful!')
        print(f'üìù Output: {result[:100]}...')
        "
    
    - name: Run unit tests
      run: |
        cd ml
        echo "üß™ Running unit tests..."
        # Add your pytest commands here when tests are available
        # pytest tests/ -v --cov=. --cov-report=term-missing
        echo "‚ö†Ô∏è  No unit tests configured yet"
    
    - name: Test summary
      if: always()
      run: |
        echo "## üìä Model Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Model Repository:** ${{ env.HF_MODEL_REPO }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Base Model:** microsoft/Phi-3-mini-4k-instruct" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
