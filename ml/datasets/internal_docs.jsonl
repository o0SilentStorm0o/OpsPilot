{"doc_id": "kb_001", "title": "Database Connection Pool Exhaustion - Runbook", "category": "Database", "content": "**Symptom**: Applications report 'Connection timeout' or 'No available connections'.\n\n**Root Cause**: Connection pool size too small for load, or connections not released properly.\n\n**Investigation Steps**:\n1. Check active connections: `SELECT count(*) FROM pg_stat_activity;`\n2. Review pool config in `database.yml` (max_connections, pool_timeout)\n3. Check for long-running queries: `SELECT * FROM pg_stat_activity WHERE state = 'active' AND query_start < now() - interval '5 minutes';`\n\n**Remediation**:\n- Immediate: Restart application to reset pool\n- Short-term: Increase pool size in config (recommended: 20-50 per instance)\n- Long-term: Implement connection pooler (PgBouncer) with transaction mode\n\n**Prevention**: Enable connection leak detection, set `idle_in_transaction_session_timeout=60s`", "tags": ["database", "postgresql", "connection-pool", "performance"]}
{"doc_id": "kb_002", "title": "BGP Route Flapping - Network SOP", "category": "Network", "content": "**Symptom**: Intermittent connectivity, route instability in BGP peers.\n\n**Root Cause**: Unstable uplink, route dampening triggered, or misconfigured route filters.\n\n**Investigation Steps**:\n1. Check BGP neighbor status: `show ip bgp summary`\n2. Review flap statistics: `show ip bgp flap-statistics`\n3. Verify route dampening config: `show ip bgp dampening parameters`\n4. Check interface errors: `show interface <uplink> | include error`\n\n**Remediation**:\n- Immediate: Clear dampening if false positive: `clear ip bgp dampening <prefix>`\n- Short-term: Adjust dampening thresholds (suppress: 2000, reuse: 800)\n- Long-term: Implement BFD (Bidirectional Forwarding Detection) for faster failover\n\n**Prevention**: Monitor route stability with Prometheus alerts on `bgp_route_changes_total` metric", "tags": ["network", "bgp", "routing", "stability"]}
{"doc_id": "kb_003", "title": "SSH Brute Force Attack - Security Playbook", "category": "Security", "content": "**Symptom**: High CPU on auth service, `/var/log/auth.log` shows repeated failed login attempts.\n\n**Root Cause**: Automated attack targeting SSH port (22) from multiple IPs.\n\n**Investigation Steps**:\n1. Count failed attempts: `grep 'Failed password' /var/log/auth.log | wc -l`\n2. Identify attacking IPs: `grep 'Failed password' /var/log/auth.log | awk '{print $(NF-3)}' | sort | uniq -c | sort -nr`\n3. Check if fail2ban is active: `systemctl status fail2ban`\n\n**Remediation**:\n- Immediate: Block attacking IPs: `iptables -A INPUT -s <IP> -j DROP`\n- Short-term: Enable fail2ban with `maxretry=3, bantime=3600`\n- Long-term: Move SSH to non-standard port, enforce key-based auth only\n\n**Prevention**: Disable password auth in `/etc/ssh/sshd_config`: `PasswordAuthentication no`, use 2FA (Google Authenticator)", "tags": ["security", "ssh", "brute-force", "fail2ban"]}
{"doc_id": "kb_004", "title": "Kubernetes OOMKilled Pods - Troubleshooting", "category": "Infrastructure", "content": "**Symptom**: Pods restarting with `OOMKilled` status, application crashes randomly.\n\n**Root Cause**: Memory limit too low, or memory leak in application.\n\n**Investigation Steps**:\n1. Check pod status: `kubectl describe pod <pod-name> | grep -A 5 'Last State'`\n2. Review resource requests/limits: `kubectl get pod <pod-name> -o yaml | grep -A 5 resources`\n3. Check memory usage: `kubectl top pod <pod-name>`\n4. Analyze heap dump (Java): `kubectl exec <pod> -- jmap -dump:file=/tmp/heap.hprof 1`\n\n**Remediation**:\n- Immediate: Increase memory limit in deployment manifest (e.g., `limits.memory: 2Gi`)\n- Short-term: Enable HPA (Horizontal Pod Autoscaler) if CPU-bound\n- Long-term: Profile application with `pprof` (Go) or VisualVM (Java), fix memory leaks\n\n**Prevention**: Set memory requests = limits for guaranteed QoS, use `oom_score_adj` tuning", "tags": ["kubernetes", "memory", "oomkilled", "troubleshooting"]}
{"doc_id": "kb_005", "title": "Redis Cache Miss Storm - Performance Guide", "category": "Performance", "content": "**Symptom**: Database load spike, slow response times, Redis GET operations return nil.\n\n**Root Cause**: Cache invalidation cascade (thundering herd), or expired keys during traffic spike.\n\n**Investigation Steps**:\n1. Check cache hit rate: `redis-cli INFO stats | grep keyspace_hits`\n2. Monitor eviction count: `redis-cli INFO stats | grep evicted_keys`\n3. Check memory usage: `redis-cli INFO memory | grep used_memory_human`\n4. Analyze slow log: `redis-cli SLOWLOG GET 10`\n\n**Remediation**:\n- Immediate: Pre-warm cache with critical keys via script\n- Short-term: Implement probabilistic early expiration (jitter on TTL)\n- Long-term: Use cache stampede protection (lock pattern with SETNX)\n\n**Prevention**: Set `maxmemory-policy allkeys-lru`, use read-through cache pattern, monitor with `cache_hit_rate` alert < 80%", "tags": ["redis", "cache", "performance", "thundering-herd"]}
{"doc_id": "kb_006", "title": "Disk Space Full - Emergency Recovery", "category": "Infrastructure", "content": "**Symptom**: Applications fail to write, logs show 'No space left on device'.\n\n**Root Cause**: Log rotation failure, large temporary files, or database growth.\n\n**Investigation Steps**:\n1. Check disk usage: `df -h`\n2. Find large files: `du -h / | sort -rh | head -20`\n3. Check inode usage: `df -i` (sometimes inodes exhausted, not space)\n4. Review log rotation: `ls -lh /var/log/*.gz | wc -l`\n\n**Remediation**:\n- Immediate: Delete old logs: `find /var/log -name '*.gz' -mtime +30 -delete`\n- Short-term: Clean package cache: `apt-get clean` or `yum clean all`\n- Long-term: Implement log shipping to centralized system (ELK, Loki)\n\n**Prevention**: Set up disk space alerts (>80%), configure logrotate properly (`rotate 7, compress`), use separate volumes for `/var/log`", "tags": ["infrastructure", "disk-space", "storage", "recovery"]}
{"doc_id": "kb_007", "title": "SSL Certificate Expiration - Renewal Process", "category": "Security", "content": "**Symptom**: HTTPS errors, browser warnings 'Your connection is not private'.\n\n**Root Cause**: SSL/TLS certificate expired, renewal automation failed.\n\n**Investigation Steps**:\n1. Check expiration date: `echo | openssl s_client -connect example.com:443 2>/dev/null | openssl x509 -noout -dates`\n2. Verify cert-manager status (K8s): `kubectl get certificate -A`\n3. Check Let's Encrypt rate limits: review acme.json logs\n\n**Remediation**:\n- Immediate: Generate temporary self-signed cert (dev only): `openssl req -x509 -nodes -days 30 -newkey rsa:2048 -keyout temp.key -out temp.crt`\n- Short-term: Manually renew with certbot: `certbot renew --force-renewal`\n- Long-term: Automate with cert-manager + Let's Encrypt in K8s\n\n**Prevention**: Set up expiration alerts (30 days before), use cert-manager with auto-renewal, monitor `ssl_cert_expiry_seconds` metric", "tags": ["security", "ssl", "tls", "certificate", "renewal"]}
{"doc_id": "kb_008", "title": "Application NullPointerException - Java Debug", "category": "Application", "content": "**Symptom**: 500 errors, stack trace shows `NullPointerException` in logs.\n\n**Root Cause**: Missing null checks, unexpected null from database/API, or race condition.\n\n**Investigation Steps**:\n1. Locate stack trace in logs: `grep -A 20 'NullPointerException' application.log`\n2. Check method signature for @Nullable annotations\n3. Review recent code changes (git blame)\n4. Enable debug logging for problematic class: `logger.setLevel(Level.DEBUG)`\n\n**Remediation**:\n- Immediate: Add null check with Optional: `Optional.ofNullable(obj).orElse(defaultValue)`\n- Short-term: Enable IDE null analysis (IntelliJ @NotNull inspection)\n- Long-term: Use Lombok @NonNull, enable NullAway static analysis\n\n**Prevention**: Adopt defensive programming, use `Objects.requireNonNull()`, enable compiler warnings (`-Xlint:nullness`), write unit tests with null inputs", "tags": ["application", "java", "npe", "debugging", "best-practices"]}
{"doc_id": "kb_009", "title": "Microservices Circuit Breaker Open - Resilience", "category": "Application", "content": "**Symptom**: Cascading failures, timeout errors, service returning fallback responses.\n\n**Root Cause**: Downstream service unhealthy, circuit breaker opened to prevent overload.\n\n**Investigation Steps**:\n1. Check circuit breaker state: `GET /actuator/health` (Spring Boot)\n2. Review Hystrix dashboard for open circuits\n3. Monitor service health: `kubectl get pods -l app=downstream-service`\n4. Check error rate: `rate(http_requests_total{status=~'5..'}[5m])`\n\n**Remediation**:\n- Immediate: Verify downstream service health, restart if needed\n- Short-term: Adjust circuit breaker thresholds (e.g., `errorThresholdPercentage=50`)\n- Long-term: Implement retry with exponential backoff, add bulkhead isolation\n\n**Prevention**: Use Resilience4j/Istio for traffic management, set up fallback responses, monitor `circuitbreaker_state` metric, test with chaos engineering (Chaos Monkey)", "tags": ["microservices", "circuit-breaker", "resilience", "fault-tolerance"]}
{"doc_id": "kb_010", "title": "Prometheus High Cardinality - Monitoring Optimization", "category": "Monitoring", "content": "**Symptom**: Prometheus OOM, slow queries, high memory usage.\n\n**Root Cause**: Too many unique label combinations (high cardinality), inefficient metric design.\n\n**Investigation Steps**:\n1. Check cardinality: `curl http://localhost:9090/api/v1/status/tsdb | jq '.data.seriesCountByMetricName'`\n2. Find top metrics: `topk(10, count by (__name__)({__name__=~'.+'}))`\n3. Identify problematic labels: use `promtool tsdb analyze /data`\n\n**Remediation**:\n- Immediate: Drop high-cardinality metrics: `metric_relabel_configs` with `action: drop`\n- Short-term: Reduce label values (e.g., hash user IDs, use buckets)\n- Long-term: Use recording rules for aggregations, implement sampling\n\n**Prevention**: Avoid unbounded labels (user_id, request_id), use `histogram_quantile()` instead of per-percentile metrics, set retention limits (`--storage.tsdb.retention.time=15d`)", "tags": ["monitoring", "prometheus", "cardinality", "optimization", "performance"]}
{"doc_id": "kb_011", "title": "Docker Build Cache Invalidation - CI/CD Optimization", "category": "Infrastructure", "content": "**Symptom**: Slow Docker builds in CI/CD, layers rebuilt unnecessarily.\n\n**Root Cause**: Inefficient Dockerfile layer ordering, COPY invalidating cache.\n\n**Investigation Steps**:\n1. Analyze build with `docker build --no-cache --progress=plain .`\n2. Check layer sizes: `docker history <image>`\n3. Review Dockerfile COPY order\n\n**Remediation**:\n- Immediate: Reorder Dockerfile: dependencies first, code last\n- Short-term: Use `.dockerignore` to exclude unnecessary files\n- Long-term: Implement multi-stage builds, use BuildKit caching\n\n**Example**:\n```dockerfile\n# ❌ Bad (invalidates cache on code change)\nCOPY . /app\nRUN npm install\n\n# ✅ Good (cache deps separately)\nCOPY package*.json /app/\nRUN npm install\nCOPY . /app\n```\n\n**Prevention**: Enable BuildKit (`DOCKER_BUILDKIT=1`), use cache mounts, pin dependency versions", "tags": ["docker", "ci-cd", "build-optimization", "cache"]}
{"doc_id": "kb_012", "title": "Kafka Consumer Lag - Streaming Data Pipeline", "category": "Infrastructure", "content": "**Symptom**: Delayed data processing, consumer lag increasing, real-time dashboards stale.\n\n**Root Cause**: Slow consumer processing, insufficient partitions, or under-resourced consumers.\n\n**Investigation Steps**:\n1. Check lag: `kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group`\n2. Monitor partition distribution: `kafka-topics --describe --topic my-topic`\n3. Check consumer CPU/memory: `kubectl top pod -l app=kafka-consumer`\n\n**Remediation**:\n- Immediate: Scale consumers horizontally (add replicas)\n- Short-term: Increase partition count (requires re-balancing): `kafka-topics --alter --partitions 20`\n- Long-term: Optimize consumer logic, use batch processing, enable compression\n\n**Prevention**: Set alerts on `kafka_consumergroup_lag > 10000`, use auto-scaling based on lag, monitor `consumer_fetch_latency_ms`", "tags": ["kafka", "streaming", "consumer-lag", "scalability"]}
{"doc_id": "kb_013", "title": "Terraform State Lock - Infrastructure Recovery", "category": "Infrastructure", "content": "**Symptom**: Terraform apply fails with 'Error acquiring state lock'.\n\n**Root Cause**: Previous operation crashed, CI/CD job killed, or manual lock not released.\n\n**Investigation Steps**:\n1. Check lock info: `terraform show` (displays lock ID and timestamp)\n2. Verify backend storage: `aws s3 ls s3://terraform-state-bucket/` (for S3 backend)\n3. Check DynamoDB lock table (AWS): `aws dynamodb get-item --table-name terraform-lock`\n\n**Remediation**:\n- Immediate: Force unlock (use with caution): `terraform force-unlock <LOCK_ID>`\n- Short-term: Verify no other operations in progress (check CI/CD)\n- Long-term: Implement state locking with timeout in backend config\n\n**Prevention**: Use remote backend with locking (S3+DynamoDB, Terraform Cloud), add `-lock-timeout=10m` to CI scripts, never run concurrent applies", "tags": ["terraform", "infrastructure-as-code", "state-lock", "recovery"]}
{"doc_id": "kb_014", "title": "API Rate Limiting - Traffic Management", "category": "Application", "content": "**Symptom**: 429 Too Many Requests errors, users complaining about blocked access.\n\n**Root Cause**: Client exceeding rate limits, DDoS attack, or misconfigured rate limiter.\n\n**Investigation Steps**:\n1. Check rate limit headers: `curl -I https://api.example.com` (X-RateLimit-Remaining)\n2. Identify top clients: `tail -1000 access.log | awk '{print $1}' | sort | uniq -c | sort -nr`\n3. Review rate limit config: `nginx.conf` (limit_req_zone) or middleware settings\n\n**Remediation**:\n- Immediate: Whitelist legitimate clients by IP or API key\n- Short-term: Implement tiered rate limits (free: 100/hour, paid: 1000/hour)\n- Long-term: Use token bucket algorithm, add Redis-based distributed rate limiting\n\n**Prevention**: Use API gateway (Kong, Envoy) with built-in rate limiting, implement exponential backoff in clients, monitor `api_rate_limit_exceeded_total` metric", "tags": ["api", "rate-limiting", "traffic-management", "security"]}
{"doc_id": "kb_015", "title": "Elasticsearch Cluster Yellow Status - Data Recovery", "category": "Database", "content": "**Symptom**: Cluster health yellow, some shards unassigned, search performance degraded.\n\n**Root Cause**: Node failure, insufficient disk space, or replica misconfiguration.\n\n**Investigation Steps**:\n1. Check cluster health: `curl -X GET 'http://localhost:9200/_cluster/health?pretty'`\n2. List unassigned shards: `curl 'localhost:9200/_cat/shards?v' | grep UNASSIGNED`\n3. Review allocation explanation: `curl -X GET 'localhost:9200/_cluster/allocation/explain?pretty'`\n\n**Remediation**:\n- Immediate: Reallocate shards manually: `curl -X POST 'localhost:9200/_cluster/reroute' -d '{\"commands\":[{\"allocate_replica\":{...}]}'`\n- Short-term: Add nodes to cluster, increase disk space\n- Long-term: Adjust replica settings: `PUT /my-index/_settings {\"number_of_replicas\": 2}`\n\n**Prevention**: Set up disk watermark alerts (85%), use index lifecycle management (ILM) to auto-delete old indices, monitor `elasticsearch_cluster_health_status` != green", "tags": ["elasticsearch", "cluster-health", "shards", "data-recovery"]}
