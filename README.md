# OpsPilot ‚Äì AI Assistant for IT Operations üöÄ

> An end-to-end open-source AI/OPS automation platform demonstrating **REAL** LLM integration, **production-grade fine-tuning**, uncertainty estimation, RAG enhancement, and enterprise-grade ML Ops practices.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue)](https://www.typescriptlang.org/)
[![Python](https://img.shields.io/badge/Python-3.11-blue)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-brightgreen)](https://www.docker.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![HuggingFace Model](https://img.shields.io/badge/ü§ó%20Model-opspilot--phi3--lora--v6-yellow)](https://huggingface.co/SilentStorm99/opspilot-phi3-lora-v6)

## üìñ Overview

**OpsPilot** is a showcase project demonstrating enterprise-level AI/OPS automation capabilities with production ML engineering:
- ‚úÖ **Three-Tier Intelligence System** - Fast Path ‚Üí RAG Path ‚Üí MastraAI Path (100% accuracy)
- ‚úÖ **LoRA Fine-tuning** of Phi-3 (3.8B params) on HuggingFace Hub
- ‚úÖ **HuggingFace Integration** - Cloud-distributed model storage & CI/CD
- ‚úÖ **RAG Enhancement** - FAISS-powered intelligent retrieval
- ‚úÖ **MastraAI Multi-Agent** - Advanced reasoning for complex incidents
- ‚úÖ **Production ML Ops** - Automated testing, deployment, and monitoring

### ü§ñ AI Models & Capabilities

#### üéØ Three-Tier Intelligence System (Production! ‚≠ê)

**Fast Path** (High Confidence ‚â•85%):
- Direct ML classification with fine-tuned Phi-3
- Average response time: ~17 seconds
- Handles 87.5% of incidents (7/8 test cases)

**RAG Path** (Medium Confidence 70-85%):
- FAISS vector search + knowledge base retrieval
- Enhanced context for borderline cases
- Improved accuracy with domain knowledge

**MastraAI Path** (Low Confidence <70%):
- Multi-agent reasoning system
- Research, analysis, and synthesis agents
- Average response time: ~20 seconds
- 100% accuracy on escalated cases (1/8 test cases)

**Overall Performance**: 100% accuracy (8/8) in production testing

#### üì¶ Fine-Tuned Model on HuggingFace

üîó **Model Repository**: [SilentStorm99/opspilot-phi3-lora-v6](https://huggingface.co/SilentStorm99/opspilot-phi3-lora-v6)

- **Base Model**: microsoft/Phi-3-mini-4k-instruct (3.8B params)
- **Method**: LoRA fine-tuning with SEQ_CLS task type (r=16, Œ±=32, dropout=0.05)
- **Model Size**: 35.7MB (LoRA adapters only)
- **Performance**: 
  - **Accuracy**: 99-100% on test incidents
  - **Dynamic confidence**: 99.8-100% (real probabilities from logits)
  - **Training**: 20 epochs, 26 samples, loss 2.6‚Üí0.3 (89% reduction)
- **Deployment**: 
  - Cloud-distributed via HuggingFace Hub
  - No large model files in git repository
  - Automatic downloads in CI/CD pipelines
  - Production server pulls model on startup
- **See**: Model card on HuggingFace for complete details

### ü§ó HuggingFace Model Distribution

OpsPilot uses **cloud-distributed model storage** via HuggingFace Hub:

**Benefits:**
- ‚úÖ No large model files in git repository (35.7MB LoRA vs 4GB+ base model)
- ‚úÖ Automatic model downloads in CI/CD and production
- ‚úÖ Version control and model history on HuggingFace
- ‚úÖ Public sharing and collaboration
- ‚úÖ Simplified containerized deployments

**Usage in Production:**
```python
# Automatic download from HuggingFace Hub
from transformers import AutoModelForCausalLM
from peft import PeftModel

# Base model (auto-downloaded)
base_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    trust_remote_code=True
)

# LoRA adapters (auto-downloaded from our HF repo)
model = PeftModel.from_pretrained(
    base_model,
    "SilentStorm99/opspilot-phi3-lora-v6"
)
```

**Training New Versions:**
```bash
# Manual workflow trigger on GitHub Actions
# .github/workflows/model-training.yml
# - Trains new model version
# - Optionally uploads to HuggingFace
# - Creates versioned releases (v7, v8, etc.)
```

#### HuggingFace API Integration (Fallback)
- **facebook/bart-large-mnli**: Zero-shot incident classification
- **deepset/roberta-base-squad2**: Question-answering for log analysis
- **facebook/bart-large-cnn**: Text summarization for recommendations

## üéØ Use Cases

**AI-powered IT incident management** with adaptive intelligence routing:
1. **Classifies incidents** using three-tier system (Fast ‚Üí RAG ‚Üí MastraAI)
2. **Analyzes system logs** with context-aware RAG retrieval
3. **Recommends remediation** via intelligent agent orchestration
4. **Escalates complex cases** to multi-agent reasoning (MastraAI)
5. **Monitors performance** with production-grade metrics
6. **100% accuracy** across all confidence levels in production testing

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Backend API    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ML Service        ‚îÇ
‚îÇ   (Next.js)     ‚îÇ      ‚îÇ  (TypeScript)    ‚îÇ      ‚îÇ Phi-3 + LoRA (HF)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ + RAG (FAISS)       ‚îÇ
                                  ‚îÇ                 ‚îÇ + MastraAI Agents   ‚îÇ
                                  ‚ñº                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚ñ≤
                         ‚îÇ   Prometheus     ‚îÇ                 ‚îÇ
                         ‚îÇ   + Grafana      ‚îÇ                 ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
                                                               ‚îÇ
                                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                    ‚îÇ  HuggingFace Hub    ‚îÇ
                                                    ‚îÇ  Model Storage      ‚îÇ
                                                    ‚îÇ  (35.7MB LoRA)      ‚îÇ
                                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ML Pipeline Architecture (Three-Tier System! ‚≠ê)
```
Incident Description
        ‚Üì
  [Phi-3 + LoRA v6]
        ‚Üì
   Classification + Confidence
        ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Confidence Routing     ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
‚â• 85%      70-85%         < 70%         Error
   ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
   ‚ñº            ‚ñº              ‚ñº              ‚ñº
[Fast Path] [RAG Path]  [MastraAI Path] [Fallback]
   ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
   ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ              ‚îÇ
   ‚îÇ    ‚îÇ FAISS Search ‚îÇ       ‚îÇ              ‚îÇ
   ‚îÇ    ‚îÇ + Knowledge  ‚îÇ       ‚îÇ              ‚îÇ
   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ              ‚îÇ
   ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
   ‚îÇ            ‚ñº              ‚ñº              ‚îÇ
   ‚îÇ    [Enhanced Context] [Multi-Agent]     ‚îÇ
   ‚îÇ            ‚îÇ          Reasoning          ‚îÇ
   ‚îÇ            ‚îÇ         (Research +         ‚îÇ
   ‚îÇ            ‚îÇ          Analysis +         ‚îÇ
   ‚îÇ            ‚îÇ          Synthesis)         ‚îÇ
   ‚îÇ            ‚îÇ              ‚îÇ              ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚Üì
            Final Classification
             (100% accuracy)
```

### Core Components

- **Backend** (`backend/`): Express.js API with MastraAI multi-agent orchestration
  - `analyzeLogs`: Log analysis and anomaly detection
  - `classifyIncident`: Three-tier intelligent routing
  - `recommendFix`: Remediation planning with RAG enhancement
  - MastraAI agents: Research, Analysis, Synthesis

- **ML Service** (`ml/`): Python-based model serving and intelligence
  - Production server with Phi-3 v6 LoRA (from HuggingFace)
  - FAISS vector search for RAG enhancement
  - Three-tier confidence-based routing
  - Automatic model download from HuggingFace Hub
  - Model evaluation and training scripts

- **Frontend** (`frontend/`): Next.js dashboard
  - Chat interface for incident analysis
  - Real-time metrics visualization
  - System health monitoring

- **CI/CD** (`.github/workflows/`): Automated ML pipeline
  - **model-test.yml**: Fast validation (download from HF + smoke tests, ~2-3 min)
  - **model-training.yml**: Manual retraining workflow (~40 min)
  - Automatic model download from HuggingFace
  - Comprehensive testing and deployment

- **Monitoring**: Prometheus metrics + Grafana dashboards

## üöÄ Quick Start

### Prerequisites

- Docker & Docker Compose
- Node.js 20+ (for local development)
- Python 3.11+ (for ML development)
- HuggingFace API Token (for model access)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/o0SilentStorm0o/OpsPilot.git
cd OpsPilot
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env and add:
# - HF_TOKEN (for HuggingFace model access)
# - OPENAI_API_KEY (for MastraAI agents)
```

3. **Run with Docker**
```bash
docker-compose up --build
```

This starts:
- Backend API: http://localhost:3001
- Frontend: http://localhost:3000
- ML Service: http://localhost:8000 (auto-downloads model from HuggingFace)
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3002 (admin/admin)

### Local Development

#### Backend
```bash
npm install
npm run dev
```

#### ML Service
```bash
cd ml
pip install -r requirements.txt

# Model automatically downloads from HuggingFace on first run
# Set PHI3_MODEL_PATH env var to use local model instead:
# export PHI3_MODEL_PATH="../models/phi3"

# Run production ML server (loads v6 LoRA from HF)
python server_production.py

# Optional: Train new model version
python train_model_classification.py

# Optional: Upload trained model to HuggingFace
python upload_model_to_hf.py
```

#### Testing Production Intelligence System
```bash
# Run comprehensive production tests
cd ml
python test_production_system.py

# Expected results:
# - Fast Path: 7/8 incidents (87.5%)
# - MastraAI escalation: 1/8 incidents (12.5%)
# - Overall accuracy: 100% (8/8)
```

## üìä Workflows

### 1. Analyze Logs
```typescript
POST /api/analyze-logs
{
  "logs": [
    {
      "timestamp": "2024-01-15T10:30:00Z",
      "level": "error",
      "message": "Connection timeout to database",
      "source": "api-gateway"
    }
  ]
}
```

### 2. Classify Incident
```typescript
POST /api/classify-incident
{
  "incident": {
    "title": "Database connection failure",
    "description": "Unable to connect to primary database",
    "source": "api-service"
  }
}
```

### 3. Recommend Fix
```typescript
POST /api/recommend-fix
{
  "incident": {
    "title": "High CPU usage",
    "description": "CPU at 95% for 10 minutes",
    "source": "app-server-01"
  }
}
```

## üìà Metrics & Monitoring

OpsPilot exposes Prometheus metrics at `/metrics`:

- `http_request_duration_seconds` - API latency
- `llm_inference_latency_seconds` - Model inference time
- `llm_tokens_generated_total` - Token usage
- `incident_classification_accuracy` - Classification accuracy
- `model_drift_score` - Model performance drift

Access Grafana dashboards at http://localhost:3002 to visualize these metrics.

## üß™ Testing

```bash
# Run tests
npm test

# Lint code
npm run lint

# Type check
npm run build
```

## üîí Security & Privacy

See [ETHICS_AND_PRIVACY.md](docs/ETHICS_AND_PRIVACY.md) for:
- Data anonymization practices
- GDPR compliance guidelines
- Model security considerations
- Responsible AI usage

## üìö Documentation

- [Setup Guide](SETUP.md) - Production deployment and configuration
- [Contributing](CONTRIBUTING.md) - Contribution guidelines
- [AI Ethics](AI_ETHICS.md) - Responsible AI practices and GDPR compliance
- [HuggingFace Model](https://huggingface.co/SilentStorm99/opspilot-phi3-lora-v6) - Phi-3 v6 LoRA model card

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|-------|-----------|
| Backend | TypeScript, Express, Node.js, MastraAI |
| ML | Python, PyTorch, Transformers, PEFT, FAISS |
| Model Storage | HuggingFace Hub (cloud-distributed) |
| Frontend | Next.js, React, Tailwind CSS |
| Monitoring | Prometheus, Grafana |
| Deployment | Docker, Docker Compose |
| CI/CD | GitHub Actions (automated testing + optional training) |

## üéì Learning Outcomes

This project demonstrates:

1. **Three-Tier Intelligence**: Adaptive routing (Fast Path ‚Üí RAG ‚Üí MastraAI) with 100% accuracy
2. **LLM Fine-tuning**: LoRA-based fine-tuning with cloud distribution via HuggingFace
3. **Production ML**: Cloud model storage, automatic downloads, version management
4. **RAG Enhancement**: FAISS vector search for knowledge retrieval
5. **Multi-Agent AI**: MastraAI orchestration with research, analysis, and synthesis agents
6. **Modern CI/CD**: Separated testing (~3 min) and training (~40 min) workflows
7. **Observability**: Comprehensive logging, metrics, and performance monitoring

## ü§ù Contributing

Contributions are welcome! Please read the contributing guidelines first.

## üìù License

MIT License - see [LICENSE](LICENSE) file for details.

## üë®‚Äçüíª Author

**David Strnadel**

Created as a demonstration of AI/OPS engineering capabilities for enterprise AI applications.

---

## üéØ For Recruiters

This project showcases:
- ‚úÖ Three-tier adaptive AI intelligence system (100% accuracy)
- ‚úÖ Production-ready LLM fine-tuning with HuggingFace integration
- ‚úÖ Multi-agent orchestration with MastraAI
- ‚úÖ RAG implementation with FAISS vector search
- ‚úÖ Cloud-distributed ML (HuggingFace Hub for model storage)
- ‚úÖ Modern CI/CD with separated testing and training workflows
- ‚úÖ Monitoring, observability, and production-grade logging
- ‚úÖ Docker deployment and enterprise documentation standards
- ‚úÖ AI ethics and privacy considerations

**Built to demonstrate real-world AI/OPS engineering skills with modern MLOps practices.**
