import { Request, Response, NextFunction } from 'express';
import promClient from 'prom-client';
import { logger } from '../utils/logger';
import { registerMastraMetrics } from './mastraMetrics';

// Create a Registry
const register = new promClient.Registry();

// Add default metrics
promClient.collectDefaultMetrics({ register });

// Custom metrics
export const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5, 10],
});

export const llmInferenceLatency = new promClient.Histogram({
  name: 'llm_inference_latency_seconds',
  help: 'Latency of LLM inference in seconds',
  labelNames: ['model', 'operation'],
  buckets: [0.5, 1, 2, 5, 10, 20, 30],
});

export const llmTokensGenerated = new promClient.Counter({
  name: 'llm_tokens_generated_total',
  help: 'Total number of tokens generated by LLM',
  labelNames: ['model', 'operation'],
});

export const incidentClassificationAccuracy = new promClient.Gauge({
  name: 'incident_classification_accuracy',
  help: 'Accuracy of incident classification (0-1)',
});

export const modelDriftScore = new promClient.Gauge({
  name: 'model_drift_score',
  help: 'Model drift score compared to baseline',
});

export const activeIncidents = new promClient.Gauge({
  name: 'active_incidents_total',
  help: 'Number of active incidents being processed',
});

// Register metrics
register.registerMetric(httpRequestDuration);
register.registerMetric(llmInferenceLatency);
register.registerMetric(llmTokensGenerated);
register.registerMetric(incidentClassificationAccuracy);
register.registerMetric(modelDriftScore);
register.registerMetric(activeIncidents);

// Register Mastra-specific metrics
registerMastraMetrics(register);
logger.info('Registered Mastra + Phi-3 Prometheus metrics');

// Middleware to track request duration
export const metricsMiddleware = (req: Request, res: Response, next: NextFunction) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestDuration
      .labels(req.method, req.route?.path || req.path, res.statusCode.toString())
      .observe(duration);
  });

  next();
};

// Metrics endpoint
export const metricsEndpoint = async (_req: Request, res: Response) => {
  try {
    res.set('Content-Type', register.contentType);
    const metrics = await register.metrics();
    res.end(metrics);
  } catch (error) {
    logger.error('Error generating metrics:', error);
    res.status(500).end();
  }
};

// Helper function to record LLM metrics
export function recordLLMMetrics(
  model: string,
  operation: string,
  latency: number,
  tokens: number
) {
  llmInferenceLatency
    .labels(model, operation)
    .observe(latency / 1000);
  
  llmTokensGenerated
    .labels(model, operation)
    .inc(tokens);
}
